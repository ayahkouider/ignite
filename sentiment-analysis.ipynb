{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setiment analysis\n- Data preprocessing\n- tokenizing\n- model building\n- Training and Evaluation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport neattext.functions as nfx\nimport nltk\nfrom nltk.corpus import twitter_samples\nfrom nltk.tokenize import word_tokenize\nfrom transformers import pipeline\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.metrics import roc_curve, auc\nimport torch \nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:53:52.461311Z","iopub.execute_input":"2024-06-23T13:53:52.462316Z","iopub.status.idle":"2024-06-23T13:54:12.593820Z","shell.execute_reply.started":"2024-06-23T13:53:52.462269Z","shell.execute_reply":"2024-06-23T13:54:12.593006Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-23 13:53:59.748190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-23 13:53:59.748300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-23 13:53:59.903305: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install neattext","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:53:33.083664Z","iopub.execute_input":"2024-06-23T13:53:33.084577Z","iopub.status.idle":"2024-06-23T13:53:47.297336Z","shell.execute_reply.started":"2024-06-23T13:53:33.084535Z","shell.execute_reply":"2024-06-23T13:53:47.296221Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting neattext\n  Downloading neattext-0.1.3-py3-none-any.whl.metadata (12 kB)\nDownloading neattext-0.1.3-py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: neattext\nSuccessfully installed neattext-0.1.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data preprocessing\n- loading data using nltk\n- Turning it into a dataframe\n- encoding the labels\n- removing special characters and stopwords\n- spliting the data","metadata":{}},{"cell_type":"code","source":"nltk.download('twitter_samples')\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:12.595671Z","iopub.execute_input":"2024-06-23T13:54:12.596612Z","iopub.status.idle":"2024-06-23T13:54:12.875938Z","shell.execute_reply.started":"2024-06-23T13:54:12.596576Z","shell.execute_reply":"2024-06-23T13:54:12.874965Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"positive=twitter_samples.strings(\"positive_tweets.json\")\nnegative=twitter_samples.strings(\"negative_tweets.json\")\nneutral=twitter_samples.strings(\"tweets.20150430-223406.json\")\n\npos_df=pd.DataFrame({'tweet': positive, 'label': 'positive'})\nneg_df=pd.DataFrame({'tweet': negative, 'label': 'negative'})\nnuet_df=pd.DataFrame({'tweet': neutral, 'label': 'neutral'})\n\ndata=pd.concat([pos_df,neg_df,nuet_df])","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:12.877210Z","iopub.execute_input":"2024-06-23T13:54:12.877518Z","iopub.status.idle":"2024-06-23T13:54:16.139817Z","shell.execute_reply.started":"2024-06-23T13:54:12.877490Z","shell.execute_reply":"2024-06-23T13:54:16.138691Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"labels={'negative': 0, 'neutral': 1, 'positive': 2}\ndata['label']=data['label'].map(labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:16.142654Z","iopub.execute_input":"2024-06-23T13:54:16.142982Z","iopub.status.idle":"2024-06-23T13:54:16.159449Z","shell.execute_reply.started":"2024-06-23T13:54:16.142953Z","shell.execute_reply":"2024-06-23T13:54:16.158474Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data['tweet'] = data['tweet'].str.replace(r'@\\w+', '', regex=True)\ndata['tweet'] = data['tweet'].str.replace(r'#\\w+', '', regex=True)\ndata['tweet'] = data['tweet'].str.replace(r'RT', '', regex=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:16.160695Z","iopub.execute_input":"2024-06-23T13:54:16.161024Z","iopub.status.idle":"2024-06-23T13:54:16.283831Z","shell.execute_reply.started":"2024-06-23T13:54:16.160998Z","shell.execute_reply":"2024-06-23T13:54:16.282408Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data['tweet']=data['tweet'].apply(nfx.remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:16.285202Z","iopub.execute_input":"2024-06-23T13:54:16.285585Z","iopub.status.idle":"2024-06-23T13:54:16.465840Z","shell.execute_reply.started":"2024-06-23T13:54:16.285553Z","shell.execute_reply":"2024-06-23T13:54:16.464649Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"],data['label'], test_size=0.3, random_state=22)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:16.467216Z","iopub.execute_input":"2024-06-23T13:54:16.467599Z","iopub.status.idle":"2024-06-23T13:54:16.483145Z","shell.execute_reply.started":"2024-06-23T13:54:16.467564Z","shell.execute_reply":"2024-06-23T13:54:16.481833Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing\n- create a dataframe for train and test sets using train_test_split output.\n- define BERT Tokenzer and create a tokenizing funtion.\n- create a tokenized train and test set.\n- add labels to tokenized datasets.\n- format tokenized datasets to be in tensors.","metadata":{}},{"cell_type":"code","source":"train_data=pd.DataFrame({'tweet':X_train,'label':y_train})\ntest_data=pd.DataFrame({'tweet':X_test,'label':y_test})","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:16.485046Z","iopub.execute_input":"2024-06-23T13:54:16.485556Z","iopub.status.idle":"2024-06-23T13:54:16.496674Z","shell.execute_reply.started":"2024-06-23T13:54:16.485518Z","shell.execute_reply":"2024-06-23T13:54:16.494712Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\ndef tokenize(examples):\n    return tokenizer(examples['tweet'], padding='max_length', truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:16.498567Z","iopub.execute_input":"2024-06-23T13:54:16.499746Z","iopub.status.idle":"2024-06-23T13:54:17.261428Z","shell.execute_reply.started":"2024-06-23T13:54:16.499681Z","shell.execute_reply":"2024-06-23T13:54:17.260672Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d91d2cfc9e946ad9b0e6c76c937c1c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdd7986d0aca435c962e68e0082d15d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae3c2962c824cdb874534933cfdc016"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d04879e34847908dba02bbbb55b569"}},"metadata":{}}]},{"cell_type":"code","source":"train_df=Dataset.from_pandas(train_data)\ntest_df=Dataset.from_pandas(test_data)\n\ntoken_train=train_df.map(tokenize,batched=True)\ntoken_test =test_df.map(tokenize,batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:17.263890Z","iopub.execute_input":"2024-06-23T13:54:17.264164Z","iopub.status.idle":"2024-06-23T13:54:42.221592Z","shell.execute_reply.started":"2024-06-23T13:54:17.264140Z","shell.execute_reply":"2024-06-23T13:54:42.220703Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b35ad06ebfa46ef84de3df4944e9420"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6eacc6c95734b0fb86354ab08f6a919"}},"metadata":{}}]},{"cell_type":"code","source":"token_train=token_train.add_column('labels',train_df['label'])\ntoken_test=token_test.add_column('labels',test_df['label'])\n\ntoken_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ntoken_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:42.222734Z","iopub.execute_input":"2024-06-23T13:54:42.223040Z","iopub.status.idle":"2024-06-23T13:54:42.385525Z","shell.execute_reply.started":"2024-06-23T13:54:42.223013Z","shell.execute_reply":"2024-06-23T13:54:42.384734Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Model building\n- load BERT model for classification.\n- define training arguments using 'TraininArguments()' function.\n- define a function for computing accuracy,precision,recall,and F1 score.\n- define trainer.","metadata":{}},{"cell_type":"code","source":"model=BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=3)\n\ntraining=TrainingArguments(\n    output_dir='./results',\n    run_name='sentiment',\n    evaluation_strategy=\"epoch\",       \n    learning_rate=2e-5,                \n    per_device_train_batch_size=8,     \n    per_device_eval_batch_size=8,      \n    num_train_epochs=5,                \n    weight_decay=0.01,                 \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:42.386607Z","iopub.execute_input":"2024-06-23T13:54:42.387015Z","iopub.status.idle":"2024-06-23T13:54:45.412079Z","shell.execute_reply.started":"2024-06-23T13:54:42.386986Z","shell.execute_reply":"2024-06-23T13:54:45.411142Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7006f810f2d84898846e1acb12272e4e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def metrics(p):\n    pred=np.argmax(p.predictions,axis=1)\n    pres,rec,f1,_=precision_recall_fscore_support(p.label_ids,pred,average='weighted')\n    acc=accuracy_score(p.label_ids,pred)\n    return{\n        'accuracy:':acc,\n        'precision:':pres,\n        'recall:':rec,\n        'f1 score:':f1\n    }\n\ntrainer=Trainer(\n   model=model,\n   args=training,\n   train_dataset=token_train,\n   eval_dataset=token_test,\n   tokenizer=tokenizer,\n   compute_metrics=metrics\n)\n   ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:45.413209Z","iopub.execute_input":"2024-06-23T13:54:45.413496Z","iopub.status.idle":"2024-06-23T13:54:46.429023Z","shell.execute_reply.started":"2024-06-23T13:54:45.413470Z","shell.execute_reply":"2024-06-23T13:54:46.428203Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Training and evaluation\n- the data trained for 5 epoch witha batch size of 8 per device.\n- training time:1 hour and 44 minutes.\n- GPU T4 x2 was used for training.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:54:46.430159Z","iopub.execute_input":"2024-06-23T13:54:46.430489Z","iopub.status.idle":"2024-06-23T15:40:19.075850Z","shell.execute_reply.started":"2024-06-23T13:54:46.430462Z","shell.execute_reply":"2024-06-23T15:40:19.074837Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240623_135516-4k83mt3g</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ayah_/huggingface/runs/4k83mt3g' target=\"_blank\">sentiment</a></strong> to <a href='https://wandb.ai/ayah_/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ayah_/huggingface' target=\"_blank\">https://wandb.ai/ayah_/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ayah_/huggingface/runs/4k83mt3g' target=\"_blank\">https://wandb.ai/ayah_/huggingface/runs/4k83mt3g</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6565' max='6565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6565/6565 1:44:41, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy:</th>\n      <th>Precision:</th>\n      <th>Recall:</th>\n      <th>F1 score:</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.012100</td>\n      <td>0.011282</td>\n      <td>0.998222</td>\n      <td>0.998234</td>\n      <td>0.998222</td>\n      <td>0.998224</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.006300</td>\n      <td>0.008720</td>\n      <td>0.998889</td>\n      <td>0.998893</td>\n      <td>0.998889</td>\n      <td>0.998890</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.003900</td>\n      <td>0.010091</td>\n      <td>0.998889</td>\n      <td>0.998893</td>\n      <td>0.998889</td>\n      <td>0.998890</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.002700</td>\n      <td>0.008979</td>\n      <td>0.999111</td>\n      <td>0.999114</td>\n      <td>0.999111</td>\n      <td>0.999112</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n      <td>0.009234</td>\n      <td>0.999111</td>\n      <td>0.999114</td>\n      <td>0.999111</td>\n      <td>0.999112</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6565, training_loss=0.008580492531933225, metrics={'train_runtime': 6332.2979, 'train_samples_per_second': 16.582, 'train_steps_per_second': 1.037, 'total_flos': 2.762690886144e+16, 'train_loss': 0.008580492531933225, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:40:19.078463Z","iopub.execute_input":"2024-06-23T15:40:19.078823Z","iopub.status.idle":"2024-06-23T15:42:56.056974Z","shell.execute_reply.started":"2024-06-23T15:40:19.078788Z","shell.execute_reply":"2024-06-23T15:42:56.056114Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [563/563 02:36]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.00923437811434269, 'eval_accuracy:': 0.9991111111111111, 'eval_precision:': 0.9991139976368262, 'eval_recall:': 0.9991111111111111, 'eval_f1 score:': 0.9991115377994043, 'eval_runtime': 156.9644, 'eval_samples_per_second': 57.338, 'eval_steps_per_second': 3.587, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}